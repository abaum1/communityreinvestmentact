{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas\n",
    "import shapely\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "bay_county_names = [\"Alameda\", \"ContraCosta\", \"Sonoma\", \"Solano\", \"SanMateo\", \"SantaClara\", \"SanFrancisco\", \"Marin\",\"Napa\"] \n",
    "analysis_years = [\"2008\", \"2009\", \"2010\", \"2011\", \"2012\", \"2013\", \"2014\", \"2015\", \"2016\", \"2017\"] #try to use 2010 census for this\n",
    "\n",
    "\n",
    "slump_years = [\"2008\", \"2009\", \"2010\", \"2011\", \"2012\"]\n",
    "recovery_years = [\"2013\", \"2014\", \"2015\", \"2016\", \"2017\"]\n",
    "geo_data_path = \"/Users/ameliabaum/Desktop/Amelia/CRA_Thesis/communityreinvestmentact/data/CTs_geo_data/\"\n",
    "parsed_data_path = \"/Users/ameliabaum/Desktop/Amelia/CRA_Thesis/communityreinvestmentact/data/parsed_data/\"\n",
    "shapefiles_data_path = \"/Users/ameliabaum/Desktop/Amelia/CRA_Thesis/communityreinvestmentact/data/raw_shapefiles/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shapefiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ameliabaum/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "#2000 census tract shapefiles (from 2008 file, but identical to 2000)\n",
    "shapefiles_08 = geopandas.read_file(shapefiles_data_path+\"alltracts_shapefiles_2008/tl_2008_06_tract00.shp\")\n",
    "bay_shapefiles_08 = shapefiles_08[shapefiles_08[\"COUNTYFP00\"].isin(['001', '013', '041', '055', '075', '081', '085', '097', '095'])]\n",
    "bay_shapefiles_08[\"NAME00\"] = pd.to_numeric(bay_shapefiles_08[\"NAME00\"])\n",
    "\n",
    "\n",
    "# Carolina says that 2010 and should be used for all HMDA data 2012-2017, but for right now I'm still using \n",
    "#2000 for these because it seems like the only one that conserves all the data in the merge\n",
    "\n",
    "# shapefiles_10 = geopandas.read_file(shapefiles_data_path+\"gz_2010_06_140_00_500k/gz_2010_06_140_00_500k.shp\")\n",
    "# bay_shapefiles_10 = shapefiles_10[shapefiles_10[\"COUNTY\"].isin(['001', '013', '041', '055', '075', '081', '085', '097', '095'])]\n",
    "# bay_shapefiles_10[\"NAME\"] = pd.to_numeric(bay_shapefiles_10[\"NAME\"])\n",
    "# bay_shapefiles_10.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging HMDA data with shapefiles for mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing... Alameda\n",
      "writing... Alameda\n",
      "writing... Alameda\n",
      "writing... Alameda\n",
      "writing... Alameda\n",
      "writing... Alameda\n",
      "writing... Alameda\n",
      "writing... Alameda\n",
      "writing... Alameda\n",
      "writing... Alameda\n",
      "writing... ContraCosta\n",
      "writing... ContraCosta\n",
      "writing... ContraCosta\n",
      "writing... ContraCosta\n",
      "writing... ContraCosta\n",
      "writing... ContraCosta\n",
      "writing... ContraCosta\n",
      "writing... ContraCosta\n",
      "writing... ContraCosta\n",
      "writing... ContraCosta\n",
      "writing... Sonoma\n",
      "writing... Sonoma\n",
      "writing... Sonoma\n",
      "writing... Sonoma\n",
      "writing... Sonoma\n",
      "writing... Sonoma\n",
      "writing... Sonoma\n",
      "writing... Sonoma\n",
      "writing... Sonoma\n",
      "writing... Sonoma\n",
      "writing... Solano\n",
      "writing... Solano\n",
      "writing... Solano\n",
      "writing... Solano\n",
      "writing... Solano\n",
      "writing... Solano\n",
      "writing... Solano\n",
      "writing... Solano\n",
      "writing... Solano\n",
      "writing... Solano\n",
      "writing... SanMateo\n",
      "writing... SanMateo\n",
      "writing... SanMateo\n",
      "writing... SanMateo\n",
      "writing... SanMateo\n",
      "writing... SanMateo\n",
      "writing... SanMateo\n",
      "writing... SanMateo\n",
      "writing... SanMateo\n",
      "writing... SanMateo\n",
      "writing... SantaClara\n",
      "writing... SantaClara\n",
      "writing... SantaClara\n",
      "writing... SantaClara\n",
      "writing... SantaClara\n",
      "writing... SantaClara\n",
      "writing... SantaClara\n",
      "writing... SantaClara\n",
      "writing... SantaClara\n",
      "writing... SantaClara\n",
      "writing... SanFrancisco\n",
      "writing... SanFrancisco\n",
      "writing... SanFrancisco\n",
      "writing... SanFrancisco\n",
      "writing... SanFrancisco\n",
      "writing... SanFrancisco\n",
      "writing... SanFrancisco\n",
      "writing... SanFrancisco\n",
      "writing... SanFrancisco\n",
      "writing... SanFrancisco\n",
      "writing... Marin\n",
      "writing... Marin\n",
      "writing... Marin\n",
      "writing... Marin\n",
      "writing... Marin\n",
      "writing... Marin\n",
      "writing... Marin\n",
      "writing... Marin\n",
      "writing... Marin\n",
      "writing... Marin\n",
      "writing... Napa\n",
      "writing... Napa\n",
      "writing... Napa\n",
      "writing... Napa\n",
      "writing... Napa\n",
      "writing... Napa\n",
      "writing... Napa\n",
      "writing... Napa\n",
      "writing... Napa\n",
      "writing... Napa\n"
     ]
    }
   ],
   "source": [
    "for county in bay_county_names:\n",
    "    for year in analysis_years:\n",
    "        parsed_df = pd.read_csv(parsed_data_path+county+'_'+year+'_parsed.csv')\n",
    "        #parsed_df[\"Geoid\"] = parsed_df[\"Geoid\"].astype(str)\n",
    "        with_geo = df.merge(bay_shapefiles_08, how=\"left\", right_on=\"NAME00\", left_on=\"Tract\")  \n",
    "        print(\"writing...\", county)\n",
    "        with_geo.to_csv(geo_data_path+county+\"_geoparsed_2008.csv\", index=False)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Recovery and Slump dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creates a combined dataframe of all the years in 2 groups, and averages the 4 proportions in this interval.\n",
    "to_concat_recovery = []\n",
    "to_concat_slump = []\n",
    "all_dfs = []\n",
    "files = [f for f in os.listdir(geo_data_path) if f[-3:] == \"csv\"]\n",
    "for name in files:\n",
    "    df_year = name.split('_')[1]\n",
    "    if df_year in slump_years:\n",
    "        df1 = pd.read_csv(geo_data_path+name)\n",
    "        all_dfs.append(df1)\n",
    "        to_concat_slump.append(df1)\n",
    "        \n",
    "    if df_year in recovery_years:\n",
    "        df2 = pd.read_csv(geo_data_path+name)\n",
    "        all_dfs.append(df2)\n",
    "        to_concat_recovery.append(df2)\n",
    "\n",
    "slump = pd.concat(to_concat_slump, axis=0).drop([\"Unnamed: 0\", \"Unnamed: 0.1\"], \n",
    "                axis=1).groupby(by=[\"Tract\", \"tract num\", \"STATEFP\", \"COUNTYFP\", \"TRACTCE\", \"AFFGEOID\", \"GEOID\", \n",
    "                                    \"NAME\", \"LSAD\", \"ALAND\", \"AWATER\", \"geometry\"]).mean()\n",
    "\n",
    "recovery = pd.concat(to_concat_recovery, axis=0).drop([\"Unnamed: 0\", \"Unnamed: 0.1\"],\n",
    "                axis=1).groupby(by=[\"Tract\", \"tract num\", \"STATEFP\", \"COUNTYFP\", \"TRACTCE\", \"AFFGEOID\", \"GEOID\", \n",
    "                                    \"NAME\", \"LSAD\", \"ALAND\", \"AWATER\", \"geometry\"]).mean()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
