{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas\n",
    "import shapely\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bay_county_names = [\"Alameda\", \"ContraCosta\", \"Sonoma\", \"Solano\", \"SanMateo\", \"SantaClara\", \"SanFrancisco\", \"Marin\",\"Napa\"] \n",
    "analysis_years = [\"2008\", \"2009\", \"2010\", \"2011\", \"2012\", \"2013\", \"2014\", \"2015\", \"2016\", \"2017\"] #try to use 2010 census for this\n",
    "\n",
    "\n",
    "slump_years = [\"2008\", \"2009\", \"2010\", \"2011\", \"2012\"]\n",
    "recovery_years = [\"2013\", \"2014\", \"2015\", \"2016\", \"2017\"]\n",
    "geo_data_path = \"/Users/ameliabaum/Desktop/Amelia/CRA_Thesis/communityreinvestmentact/data/CTs_geo_data_1/\"\n",
    "parsed_data_path = \"/Users/ameliabaum/Desktop/Amelia/CRA_Thesis/communityreinvestmentact/data/parsed_data_1/\"\n",
    "shapefiles_data_path = \"/Users/ameliabaum/Desktop/Amelia/CRA_Thesis/communityreinvestmentact/data/raw_shapefiles/\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shapefiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ameliabaum/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "#2000 census tract shapefiles (from 2008 file, but identical to 2000)\n",
    "shapefiles_08 = geopandas.read_file(shapefiles_data_path+\"alltracts_shapefiles_2008/tl_2008_06_tract00.shp\")\n",
    "bay_shapefiles_08 = shapefiles_08[shapefiles_08[\"COUNTYFP00\"].isin(['001', '013', '041', '055', '075', '081', '085', '097', '095'])]\n",
    "bay_shapefiles_08[\"NAME00\"] = pd.to_numeric(bay_shapefiles_08[\"NAME00\"])\n",
    "\n",
    "\n",
    "# Carolina says that 2010 and should be used for all HMDA data 2012-2017, but for right now I'm still using \n",
    "#2000 for these because it seems like the only one that conserves all the data in the merge\n",
    "\n",
    "# shapefiles_10 = geopandas.read_file(shapefiles_data_path+\"gz_2010_06_140_00_500k/gz_2010_06_140_00_500k.shp\")\n",
    "# bay_shapefiles_10 = shapefiles_10[shapefiles_10[\"COUNTY\"].isin(['001', '013', '041', '055', '075', '081', '085', '097', '095'])]\n",
    "# bay_shapefiles_10[\"NAME\"] = pd.to_numeric(bay_shapefiles_10[\"NAME\"])\n",
    "# bay_shapefiles_10.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging HMDA data with shapefiles for mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing... Alameda 2008\n",
      "writing... Alameda 2009\n",
      "writing... Alameda 2010\n",
      "writing... Alameda 2011\n",
      "writing... Alameda 2012\n",
      "writing... Alameda 2013\n",
      "writing... Alameda 2014\n",
      "writing... Alameda 2015\n",
      "writing... Alameda 2016\n",
      "writing... Alameda 2017\n",
      "writing... ContraCosta 2008\n",
      "writing... ContraCosta 2009\n",
      "writing... ContraCosta 2010\n",
      "writing... ContraCosta 2011\n",
      "writing... ContraCosta 2012\n",
      "writing... ContraCosta 2013\n",
      "writing... ContraCosta 2014\n",
      "writing... ContraCosta 2015\n",
      "writing... ContraCosta 2016\n",
      "writing... ContraCosta 2017\n",
      "writing... Sonoma 2008\n",
      "writing... Sonoma 2009\n",
      "writing... Sonoma 2010\n",
      "writing... Sonoma 2011\n",
      "writing... Sonoma 2012\n",
      "writing... Sonoma 2013\n",
      "writing... Sonoma 2014\n",
      "writing... Sonoma 2015\n",
      "writing... Sonoma 2016\n",
      "writing... Sonoma 2017\n",
      "writing... Solano 2008\n",
      "writing... Solano 2009\n",
      "writing... Solano 2010\n",
      "writing... Solano 2011\n",
      "writing... Solano 2012\n",
      "writing... Solano 2013\n",
      "writing... Solano 2014\n",
      "writing... Solano 2015\n",
      "writing... Solano 2016\n",
      "writing... Solano 2017\n",
      "writing... SanMateo 2008\n",
      "writing... SanMateo 2009\n",
      "writing... SanMateo 2010\n",
      "writing... SanMateo 2011\n",
      "writing... SanMateo 2012\n",
      "writing... SanMateo 2013\n",
      "writing... SanMateo 2014\n",
      "writing... SanMateo 2015\n",
      "writing... SanMateo 2016\n",
      "writing... SanMateo 2017\n",
      "writing... SantaClara 2008\n",
      "writing... SantaClara 2009\n",
      "writing... SantaClara 2010\n",
      "writing... SantaClara 2011\n",
      "writing... SantaClara 2012\n",
      "writing... SantaClara 2013\n",
      "writing... SantaClara 2014\n",
      "writing... SantaClara 2015\n",
      "writing... SantaClara 2016\n",
      "writing... SantaClara 2017\n",
      "writing... SanFrancisco 2008\n",
      "writing... SanFrancisco 2009\n",
      "writing... SanFrancisco 2010\n",
      "writing... SanFrancisco 2011\n",
      "writing... SanFrancisco 2012\n",
      "writing... SanFrancisco 2013\n",
      "writing... SanFrancisco 2014\n",
      "writing... SanFrancisco 2015\n",
      "writing... SanFrancisco 2016\n",
      "writing... SanFrancisco 2017\n",
      "writing... Marin 2008\n",
      "writing... Marin 2009\n",
      "writing... Marin 2010\n",
      "writing... Marin 2011\n",
      "writing... Marin 2012\n",
      "writing... Marin 2013\n",
      "writing... Marin 2014\n",
      "writing... Marin 2015\n",
      "writing... Marin 2016\n",
      "writing... Marin 2017\n",
      "writing... Napa 2008\n",
      "writing... Napa 2009\n",
      "writing... Napa 2010\n",
      "writing... Napa 2011\n",
      "writing... Napa 2012\n",
      "writing... Napa 2013\n",
      "writing... Napa 2014\n",
      "writing... Napa 2015\n",
      "writing... Napa 2016\n",
      "writing... Napa 2017\n"
     ]
    }
   ],
   "source": [
    "for county in bay_county_names:\n",
    "    for year in analysis_years:\n",
    "        parsed_df = pd.read_csv(parsed_data_path+county+'_'+year+'_parsed.csv')\n",
    "        with_geo = parsed_df.merge(bay_shapefiles_08, how=\"left\", right_on=\"NAME00\", left_on=\"Tract\")  \n",
    "        print(\"writing...\", county, year)\n",
    "        with_geo.to_csv(geo_data_path+county+\"_geoparsed_\"+year+\".csv\", index=False)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Recovery and Slump dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates a combined dataframe of all the years in 2 groups, and averages the 4 proportions in this interval.\n",
    "to_concat_recovery = []\n",
    "to_concat_slump = []\n",
    "all_dfs = []\n",
    "files = [f for f in os.listdir(geo_data_path) if f[-3:] == \"csv\"]\n",
    "for name in files:\n",
    "    df_year = name.split('_')[2][:4]\n",
    "    if df_year in slump_years:\n",
    "        df1 = pd.read_csv(geo_data_path+name)\n",
    "        all_dfs.append(df1)\n",
    "        to_concat_slump.append(df1)\n",
    "        \n",
    "    if df_year in recovery_years:\n",
    "        df2 = pd.read_csv(geo_data_path+name)\n",
    "        all_dfs.append(df2)\n",
    "        to_concat_recovery.append(df2)\n",
    "\n",
    "slump = pd.concat(to_concat_slump, axis=0)\n",
    "#.groupby(by=[\"Tract\", \"tract num\", \"STATEFP\", \"COUNTYFP\", \"TRACTCE\", \"AFFGEOID\", \"GEOID\", \n",
    "#                                    \"NAME\", \"LSAD\", \"ALAND\", \"AWATER\", \"geometry\"]).mean()\n",
    "\n",
    "recovery = pd.concat(to_concat_recovery, axis=0)\n",
    "#.groupby(by=[\"Tract\", \"tract num\", \"STATEFP\", \"COUNTYFP\", \"TRACTCE\", \"AFFGEOID\", \"GEOID\", \n",
    "#                                    \"NAME\", \"LSAD\", \"ALAND\", \"AWATER\", \"geometry\"]).mean()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7143\n",
      "6779\n"
     ]
    }
   ],
   "source": [
    "print(len(slump))\n",
    "print(len(slump.dropna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "slump.to_csv(geo_data_path+\"all_tracts_geoparsed_slump.csv\", index=False)\n",
    "recovery.to_csv(geo_data_path+\"all_tracts_geoparsed_recovery.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
